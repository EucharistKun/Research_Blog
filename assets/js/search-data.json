{
  
    
        "post0": {
            "title": "Applying Human Pose Estimation to Full Body Xrays",
            "content": "",
            "url": "https://eucharistkun.github.io/Research_Blog/jupyter/2021/08/25/Human-Pose-Estimation.html",
            "relUrl": "/jupyter/2021/08/25/Human-Pose-Estimation.html",
            "date": " • Aug 25, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "The UK Biobank DXA Image Quality Control Process",
            "content": "Problem 1 - What File Name Corresponds to What Image? . All DXA Images were obtained from the UK Biobank bulk field ID 20158 in the format of Patient-EID_Bulk-field-ID_Visit-Number.zip (6024685_20158_2_0.zip). These folders typically had 8 images, labeled in this format: . Problem 2 - Full Body Xrays are Poorly Cropped . After the initial QC process and dividing up the images into a folder for each body part, I began to look at the full body transparent xrays in preparation for our project involving Human Pose Estimation (HPE). Many of the full body xrays had parts of the arm cut off from the final image, resulting in poor pose estimation from the deep learning architecture (HRnet/Resnet). In order to remove images that were cut off at the arm, I created a binary classifier to distinguish between the two types of images and applied it to the entire dataset of ~40,000 images. . Problem 3 - Xray Sizes Vary a Lot .",
            "url": "https://eucharistkun.github.io/Research_Blog/jupyter/2021/07/15/The-UKBiobank-DXA-Image-Quality-Control-Process.html",
            "relUrl": "/jupyter/2021/07/15/The-UKBiobank-DXA-Image-Quality-Control-Process.html",
            "date": " • Jul 15, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "FastAI and Machine Learning",
            "content": "FastAI Chapter 1 . I have been learning ML through use of the FastAI library, which is built upon PyTorch. Here I will define the basics of machine learning as used in a binary classification model as per the fastbook lesson 1. Programming generally involves a flow chart of inputs -&gt; program -&gt; results in order to solve a problem. Machine learning involves teaching the computer to solves the problems itself by adding a few things to the work flow. By adding &quot;weights&quot; (or parameters) to the program (or model), one can adjust the results of the model even with the same inputs. According to fastai, &quot;weights are just variables, and a weight assignment is a particular choice of values for those variables.&quot; .",
            "url": "https://eucharistkun.github.io/Research_Blog/jupyter/2021/06/15/Introduction-to-Machine-Learning-Through-FastAI.html",
            "relUrl": "/jupyter/2021/06/15/Introduction-to-Machine-Learning-Through-FastAI.html",
            "date": " • Jun 15, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "GWAS, PheWAS, and Mendelian Randomization",
            "content": "GWAS . Genome Wide Association Studies (GWAS) look for genetic variants across the genome in a large amount of individuals to see if any variants are associated with a specific trait such as height or disease. GWA studies typically look at single nucleotide polymorphisms (SNPs), which are germline substitutions of a single nucleotide at a specific position in the genome, meaning that these are heritable differences in the human population. A GWAS is performed by taking DNA samples from many individuals and using SNP arrays to read the different genetic variants. If a particular variant is more present in people with a specific trait, the SNP is associated with the disease. Results from a GWAS are typically shown in a Manhattan plot displaying which loci on the various chromosomes are more associated with a specific trait. In the picture below taken from Wikipedia, each dot represents a SNP, and &quot;this example is taken from a GWA study investigating microcirculation, so the tops indicates genetic variants that more often are found in individuals with constrictions in small blood vessels.&quot; . . A GWAS is a non-candidate-driven approach, in that a GWAS investigate the whole genome and not specific genes. As a result, a GWAS can tell the user which genes are associated with the disease but cannot describe any causal relations between the genes identified and the trait/disease being studied. . Methodology . Typically, two general populations are used for a GWAS, a case group with a certain disease and a control group without the disease. The individuals in the population are genotyped for the majority of known SNPs in the human genome, which surpass a million. The allele frequences at each SNP are calculated, and an odds ratio is generated in order to compare the case and control populations. An odds ratio is &quot;the ratio of the odds of A in the presence of B and the odds of A in the absence of B&quot;, or in the case of a GWAS, it is &quot;the odds of case for individuals having a specific allele and the odds of case for individuals who do not have that same allele&quot;. . For example, at a certain SNP, there are two main alleles, T and C. The amount of individuals in the case group having allele T is represented by A, or 4500, and the amount of individuals in the control group having allele T is represented by B, or 2000. Then the number of individuals in the case group having allele C is represented by X, or 3000, and the individuals in the control group having allele C is represented by Y, or 3500. The odds ratio for allele T is calculated as (A/B) / (X/Y) or (4500/2000) / (3000,3500). . If the allele frequency is much higher in the case group than in the control group, the odds ratio will be greater than one. Furthermore, a chi-squared test is used to calculate a p-value for the significance of the generated odds ratios. For a GWAS, typically a p-value &lt; 5x10^-8 is required for an odds ratio to be meaningful. . Furthermore, factors such as ethnicity, geography, sex, and age must be taken into consideration and controlled for as they could confound the results. . Imputation . Another key facet used in many studies involves imputation, or the statistical inference of unobserved genetic sequences. Since it is time-consuming and costly to do genome wide sequencing on a large population, only key areas of the genome are typically sequenced and a large portion of the genome is statistically inferred through large scale genome datasets such as the HapMap or 1000 Genomes Project. Imputation is achieved by combining the GWAS data with the reference panel of haplotypes (HapMap/1000 Genomes Project) and inferring other SNPs in the genome through shared haplotypes among individuals over short sequences. For example, if we know that a patient with an A allele at base 10 always has a G allele at base 135, we can impute this information for the entire population. This method increases the number of SNPs that can be tested for association in a GWAS as well as the power of the GWAS. .",
            "url": "https://eucharistkun.github.io/Research_Blog/jupyter/2021/05/24/Understanding-Various-Genetic-Analyses.html",
            "relUrl": "/jupyter/2021/05/24/Understanding-Various-Genetic-Analyses.html",
            "date": " • May 24, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Building My First Binary Image Classifier",
            "content": "Hips and Knees and Spines, Oh My . The crux of my first project in this lab involved taking patients from the UK Biobank who had been diagnosed with hip osteoarthritis (OA) and attempting to train a classifier using machine learning (ML) on X-rays of these patients&#39; hips. Hip OA is typically diagnosed by looking at X-rays of the hip, specifically looking at the hip-joint space between the hip and the pelvis as well as other diagnostic markers more clearly described here. In the image below, the hip on the left side of the picture has OA while the right hip is normal. . . An added layer of depth to this project involved seeing if we could look at other X-rays or images from other body parts of these patients not typically used in diagnosing hip OA and whether or not these additional images could give some additional diagnostic clues or outcome predictors for hip OA. So if, for example, looking at only a hip X-ray for diagnosing hip OA had generally 85% accuracy, could looking at an X-ray of the spine improve diagnostic accuracy or tell us something about potential health outcomes for the patient. Though it seems unlikely for this particular disease, this line of thinking could be broadly applied to a variety of diseases. Say liver cancer is typically diagnosed through a liver MRI, but what if taking a brain CT scan alongside a liver MRI gave earlier diagnostic accuracy or could predict disease severity? Thinking along these lines, I set out to create and attempt to understand my first classifier - whether or not my model could predict if a patient had hip OA or not based upon just a hip X-ray. . Obtaining the Images and Organizing Files . All DXA Images were obtained from the UK Biobank. Additionally, metadata files corresponding to patient hip OA diagnosis and other metadata were downloaded from the Biobank. A previous lab member had already obtained a set of 200 patients who had been diagnosed with hip OA and 200 patients without the diagnosis. I placed these patient folders containing the X-ray images into two separate folders, one labeled OA_200 and another labeled Not_OA_200. In order to distinguish between patients with hip OA versus those without, I wrote a bash script to rename all the patient folders in the OA_200 directory. . (Ex: regular patient = 1000000_20158_2_0.zip, OA patient = OA_1000001_20158_2_0.zip) . #!/bin/bash # This file will rename the OA zip files to include OA in the name echo &quot;Rename OA zip files so they begin with OA&quot; for f in *.zip; do nf=&quot;OA_&quot;${f} mv -- &quot;$f&quot; &quot;$nf&quot; done . I then unzipped the files using the code I wrote about in my last blog post. From here, I followed a fastAI tutorial about binary classification of medical images and mimicked their workflow entirely. . Dicom Files - An Aside . &quot;DICOM(Digital Imaging and COmmunications in Medicine) is the de-facto standard that establishes rules that allow medical images(X-Ray, MRI, CT) and associated information to be exchanged between imaging equipment from different vendors, computers, and hospitals. The DICOM format provides a suitable means that meets health infomation exchange (HIE) standards for transmision of health related data among facilites and HL7 standards which is the messaging standard that enables clinical applications to exchange data . DICOM files typically have a .dcm extension and provides a means of storing data in separate ‘tags’ such as patient information as well as image/pixel data. A DICOM file consists of a header and image data sets packed into a single file. By extracting data from these tags one can access important information regarding the patient demographics, study parameters, etc.&quot; . Building The Classifier . First, a few Python libraries needed to be installed and imported to TACC. These libraries below are necessary for viewing dicom images and running the classifier according to the fastAI tutorial . #!pip install pydicom kornia opencv-python scikit-image nbdev #from fastai.basics import * #from fastai.callback.all import * #from fastai.vision.all import * #from fastai.medical.imaging import * #import pydicom . The following libraries were used to easily navigate the directories in TACC and generate a csv from a pandas dataframe in order to mimic the file layout in the fastAI tutorial . #import pandas as pd #import os #import glob . My directory layout is as follows: . . Based upon the file name which follows a certain nomenclature for determining what dicom image corresponds to what body part, I pulled only left hip images from the OA_200 and Not_OA_200 directories into LeftHipImages. When I pulled the images from OA_200 to the new directory, I renamed the files to begin with &quot;OAOA&quot; so that I could generate a label based upon filename for whether the image had a hip OA diagnosis or not. I used the following code to generate a pandas df which I turned into the OA_Labels.csv file. . # Puts all dicom images in HipImages into a list #files = (glob.glob(&quot;/work2/08068/ekun/frontera/OA_Hip_Files/LeftHipImages/*.dcm&quot;)) # Creates a dataframe and populates the first column &quot;File&quot; with the file names of the dicom images #df = pd.DataFrame() #df[&quot;File&quot;] = files # Looks at the file column in my dataframe and labels &quot;OA&quot; or &quot;Not OA&quot; accordingly # depending on if the file has &quot;OA_OA_&quot; in the name or not #labels = [] #for image in df[&quot;File&quot;]: # if &quot;OA_OA_&quot; in image: # labels.append(&quot;OA&quot;) # else: # labels.append(&quot;Not_OA&quot;) #df[&quot;Label&quot;] = labels # Writes the dataframe to a csv file #df.to_csv(&quot;/work2/08068/ekun/frontera/OA_Hip_Files/OA_Labels.csv&quot;, sep=&#39; t&#39;, index = False) . The resulting dataframe looked like this . . Most of the rest of the code was directly taken from the fastAI tutorial and is presented below . # Path to the Hip Images #path = Path(&quot;/work2/08068/ekun/frontera/OA_Hip_Files&quot;) # Obtain Dicom Images #items = get_dicom_files(&quot;/work2/08068/ekun/frontera/OA_Hip_Files/LeftHipImages&quot;) # Split images into training and validation set, the seed is set to 42 so the images will be split into the same groups everytime #trn,val = RandomSplitter(seed=42)(items) #OA = DataBlock(blocks=(ImageBlock(cls=PILDicom), CategoryBlock), # get_x=lambda x:path/f&quot;{x[0]}&quot;, # get_y=lambda x:x[1], # item_tfms=Resize(224), # batch_tfms=[*aug_transforms(size=224, do_flip=False),Normalize.from_stats(*imagenet_stats)]) . #dls = OA.dataloaders(df.values, num_workers=0) #dls.show_batch(max_n=64) . . #learn = cnn_learner(dls, resnet152, loss_func=CrossEntropyLossFlat(), metrics=accuracy) #learn.fine_tune(5) . . #learn.show_results(max_n=32, nrows=8) . . #interp = ClassificationInterpretation.from_learner(learn) #losses,idxs = interp.top_losses() #len(dls.valid_ds)==len(losses)==len(idxs) #interp.plot_confusion_matrix(figsize=(7,7)) . . #interp.plot_top_losses(32, nrows=8) . . What Went Wrong . Clearly, the classifier did not have a very high accuracy in determining hip OA based on hip X-rays. However, any trained doctor is generally able to diagnose hip OA based upon X-rays alone. One extremely noteable source of errors lies in the fact that the metadata obtained for each patient did not specify which hip had OA. OA typically only presents itself in a single hip for each patient, and without that information, some of these hip X-rays were misleading as they could be fully healthy and not the correct hip. Furthermore, it is possible that this image classification is not as overt as we originally thought. However, that does not explain the extremely low accuracy that my classifier generated after training. . Moving Forward . I made two more classifiers, one for left knees and one for spine X-rays following the exact same file structure layout and code as this classifier in order to see if there were any differences in accuracy. Without any meaningful OA diagnosis information, each classifier had about the same accuracy as this classifier, around 50%, no better than a coin flip. It was at this point that we decided to abandon the project for now, but the idea here could be broadly applied to different diseases and medical images. .",
            "url": "https://eucharistkun.github.io/Research_Blog/jupyter/2021/05/16/My-First-Binary-Image-Classifier.html",
            "relUrl": "/jupyter/2021/05/16/My-First-Binary-Image-Classifier.html",
            "date": " • May 16, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "My Early Journey in TACC, Navigating Linux, and More",
            "content": "The Workflow Layout . All computational work is done on the TACC supercomputer at UT. I was allocated to the Frontera machine, and my first task was to ssh into the supercomputer so that I could set up my files and directories in order to get working. . ssh -X ekun@frontera.tacc.utexas.edu . The main directories I use are . /home1/08068/ekun | . and . /work2/08068/ekun/frontera | . as well as . /scratch1/08068/ekun | . The main website used to create Jupyter Notebooks for FastAI and creating my own classifiers. All Jupyter Notebooks created on vis.tacc.utexas.edu are stored on the home directory . All UKBiobank directory metadata and images are located at . /corral-repl/utexas/UKB-Imaging-Genetics/ | . This repo has a lot which I still need to explore, but all the DXA Images are located under . /corral-repl/utexas/UKB-Imaging-Genetics/Imaging_Data/DXA/DXA_Images | . This folder has all patient EID zip files with their images. . /corral-repl/utexas/UKB-Imaging-Genetics/unzipped_DXA_Images | . This folder has all patient EID files unzipped with their images as well as DXA images by body part . Other Directories of Interest: . Generated CSV files are stored in . /work2/08068/ekun/frontera/output_files | . Currently a copy of all unzipped DXA images and the images separated by body parts are located in . /scratch1/08068/ekun/unzipped_DXA_Images | . Navigating The Linux Terminal . My first issues arose after ssh-ing into TACC as the TACC UI is a Linux command line terminal. The most useful commands I learned were as follows: . cd /directory - Enters the directory specified | cd .. - Navigates back a directory | pwd - Displays path of current directory | ls - Displays files of current directory | ls -la - Displays all files of current directory and additional information | wc -l /file - gives line count of a file ls | wc -l allows you to pipe the line count command to the directory, giving the file count of the directory | . | head filename.txt or .csv - Displays first few lines of most files | less -S filename.txt or .csv - Allows you to easily view files | vim filename.txt or .sh allows you to begin writing a file whether it be a text file or bash script i - allows you to start writing inside vim | :wq - save and close the file | . | scp -r /directory /destination - recursively copies a directory or file to a destination | rm /file - deletes a file rm -r - recursively deletes a folder and everything inside | . | grep &quot;term&quot; /file - searches a file for given term ls | grep &quot;term&quot; - searches current directory for a given term | . | . Writing My First Bash Scripts . The DXA Images were all stored in .zip files with the following syntax: PatientEID_DataField_2_0.zip (ex: 1003186_20158_2_0.zip) . I first had to write a bash script to unzip the files and rename the folders containing the images to retain the patient EID. (ex: 1003186_20158_2_0_unzip) . filename unzip.sh #!/bin/bash echo &quot;This unzips all the files in the directory and puts the unzipped files into directories named after the original zip folder&quot; for f in *.zip; do unzip -d &quot;${f%.zip}_unzip&quot; &quot;$f&quot; done # To unzip the files and put them into a new directory outside of the current directory while renaming them: for f in *.zip; do unzip -d &quot;/corral-repl/utexas/UKB-Imaging-Genetics/unzipped_DXA_Images/${f%.zip}_unzip&quot; &quot;$f&quot; done .",
            "url": "https://eucharistkun.github.io/Research_Blog/jupyter/2021/04/30/My-First-Narasimhan-Lab-Blog.html",
            "relUrl": "/jupyter/2021/04/30/My-First-Narasimhan-Lab-Blog.html",
            "date": " • Apr 30, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://eucharistkun.github.io/Research_Blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://eucharistkun.github.io/Research_Blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a current graduate student in the Narasimhan Lab at the University of Texas at Austin looking to expand my knowledge of biology, genetics, and computer science in order to improve cancer research. I joined this lab in March of 2021 after hearing of a project that Vagheesh wanted to undertake. To directly pull from Vagheesh’s website, “we are building machine learning methods, particularly deep learning approaches to automatically extract features and phenotypes from a variety of biomedical imaging datasets and then connecting these with genomic information to understand the genetic basis of these traits.” It sounded like a daunting task as a biochemistry graduate student with a limited background in computer science, but I was intrigued and felt like this was an area of up and coming interest that could have potentially powerful applications for medical and in particular, cancer research. . As a child, I wasn’t always sure what I would end up doing when I grew up, but two experiences with Acute Lymphoblastic Leukemia, once at age 3 and again at the age of 12, led me to pursure a degree in biochemistry at the University of Texas at Austin in order to do cancer research. A course in bioinformatics in undergrad also prompted me to delve into my interest in computer science, so I completed the Elements of Computing Certificate at UT as well. . After completing my degree, my dreams of doing cancer research were realized as I began a research assistant role at MD Anderson Cancer Center in Houston, Texas. I worked under Dr. Kwong Kwok Wong in the Department of Gynecology Oncology and Reproductive Medicine. Our lab focused on Low Grade Serous Ovarian Cancer, a particularly deadly and rare form of ovarian cancer that affected primarily younger women. During my time at MD Anderson, I learned a lot more about the difficulty of treating cancer and the different avenues of cancer research that were available. Reinforced in my drive to pursue cancer research, I applied for various PhD programs in 2019 and accepted an offer at the University of Texas at Austin in Biochemistry starting in the fall of 2020. . Moving on from my professional background, I find it difficult to explain my interests and hobbies so I will let my Youtube Recommended Video Categories describe them for me. . “Breaking Bad” | “Michael Scott” | “Super Smash Bros Melee” | “Bodybuilding” | “Hunter X Hunter” | “NBA” | . That honestly sounds about right despite never watching a full episode of The Office in my life. I have also recently attempted to up my photography and editing skills which I showcase here . This page is a work in progress .",
          "url": "https://eucharistkun.github.io/Research_Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://eucharistkun.github.io/Research_Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}